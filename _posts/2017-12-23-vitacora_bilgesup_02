---
published: false
---
## A New Post

Enter text in [Markdown](http://daringfireball.net/projects/markdown/). Use the toolbar above, or click the **?** button for formatting help.


Revisando sobre las preguntas que quedaron me puse a averiguar sobre la normalizacion de entradas y salidas

http://www.faqs.org/faqs/ai-faq/neural-nets/part2/
http://yeephycho.github.io/2016/08/03/Normalizations-in-neural-networks/
https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network

Otra cosa que me puse a pensar es que quiza estoy dejando mal configurado el learning rate. Este deberia bajar si se entrenara continuamente, sin embargo estoy re iniciando el entrenamiento cada cierto tiempo lo cual podria inducir saltos en el learning rate.

Efectivamente estaba utilizando GradientDescentOptimizer con un learning rate fijo. 

Experimentare con un learning rate que decae exponencialmente:

decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)



